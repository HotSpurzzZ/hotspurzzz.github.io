<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="论文阅读 -- &quot;Kernel k-means, Spectral Clustering and Normalized Cuts&quot;"><meta name="keywords" content="Paper Reading,Data mining"><meta name="author" content="HotSpurzzZ"><meta name="copyright" content="HotSpurzzZ"><title>论文阅读 -- &quot;Kernel k-means, Spectral Clustering and Normalized Cuts&quot; | HotSpurzzZ</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"atom.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '5.4.0'
} </script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="HotSpurzzZ" type="application/atom+xml">
</head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="false"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E6%8F%90%E7%9F%A5%E8%AF%86"><span class="toc-number">1.</span> <span class="toc-text">前提知识</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#k-means%EF%BC%88K%E5%9D%87%E5%80%BC%EF%BC%89"><span class="toc-number">1.1.</span> <span class="toc-text">k-means（K均值）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kernel-k-means%EF%BC%88%E6%A0%B8k-means%E6%96%B9%E6%B3%95%EF%BC%89"><span class="toc-number">1.2.</span> <span class="toc-text">Kernel k-means（核k-means方法）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#weighted-kernel-k-means"><span class="toc-number">1.2.1.</span> <span class="toc-text">weighted kernel k-means</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spectral-Clustering%EF%BC%88%E8%B0%B1%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95%EF%BC%89"><span class="toc-number">1.3.</span> <span class="toc-text">Spectral Clustering（谱聚类方法）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%91%98%E8%A6%81%E4%B8%8E%E8%83%8C%E6%99%AF"><span class="toc-number">2.</span> <span class="toc-text">摘要与背景</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Weighted-Kernel-k-means"><span class="toc-number">3.0.1.</span> <span class="toc-text">Weighted Kernel k-means</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spectral-clustering-%E8%B0%B1%E8%81%9A%E7%B1%BB"><span class="toc-number">3.0.2.</span> <span class="toc-text">Spectral clustering(谱聚类)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#THE-SPECTRAL-CONNECTION"><span class="toc-number">4.</span> <span class="toc-text">THE SPECTRAL CONNECTION</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#IMPLICATIONS"><span class="toc-number">5.</span> <span class="toc-text">IMPLICATIONS</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Normalized-Cuts-using-Weighted-Kernel-k-means"><span class="toc-number">5.0.1.</span> <span class="toc-text">Normalized Cuts using Weighted Kernel k-means</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kernel-k-means-using-Eigenvectors"><span class="toc-number">5.0.2.</span> <span class="toc-text">Kernel k-means using Eigenvectors</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Interpreting-NJW-As-Kernel-k-means"><span class="toc-number">5.0.3.</span> <span class="toc-text">Interpreting NJW As Kernel k-means</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#EXPERIMENTAL-RESULTS"><span class="toc-number">6.</span> <span class="toc-text">EXPERIMENTAL RESULTS</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">6.1.</span> <span class="toc-text">数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C"><span class="toc-number">6.2.</span> <span class="toc-text">结果</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Diametrical-Clustering-of-Gene-Expression-Data%EF%BC%88%E5%9F%BA%E5%9B%A0%E8%A1%A8%E8%BE%BE%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%EF%BC%89"><span class="toc-number">6.2.1.</span> <span class="toc-text">Diametrical Clustering of Gene Expression Data（基因表达数据的聚类）:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Clustering-Handwriting-Recognition-Data-Set%EF%BC%88%E8%81%9A%E7%B1%BB%E7%AC%94%E8%BF%B9%E8%AF%86%E5%88%AB%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%89"><span class="toc-number">6.2.2.</span> <span class="toc-text">Clustering Handwriting Recognition Data Set（聚类笔迹识别数据集）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Conclusion"><span class="toc-number">7.</span> <span class="toc-text">Conclusion</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://i.loli.net/2021/09/28/5Oing3pmAYI6VtD.jpg"></div><div class="author-info__name text-center">HotSpurzzZ</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">23</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">25</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">11</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">HotSpurzzZ</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">主页</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">论文阅读 -- &quot;Kernel k-means, Spectral Clustering and Normalized Cuts&quot;</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2022-04-20</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Data-mining/">Data mining</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>Introducetion: 数据挖掘：k-means聚类介绍<br>Topic: Data Mining<br>URL: <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/1014052.1014118">https://dl.acm.org/doi/abs/10.1145/1014052.1014118</a><br>Where: KDD<br>Year: 2004</p>
<span id="more"></span>
<p>本文只是学习并记录笔记，如有错误或不足请谅解指正，谢谢！</p>
<h1 id="前提知识"><a href="#前提知识" class="headerlink" title="前提知识"></a>前提知识</h1><hr>
<h2 id="k-means（K均值）"><a href="#k-means（K均值）" class="headerlink" title="k-means（K均值）"></a>k-means（K均值）</h2><p>是基于数据划分的无监督聚类算法</p>
<p>聚类算法：可以理解为无监督的分类方法</p>
<ul>
<li>基本原理<ul>
<li>对于基于划分的这一类聚类算法而言，其方法是将样本的矢量空间划分为多个区域 $<code>&#123;&#123;S_i|^k_&#123;i=1&#125;&#125;&#125;</code>$，每个区域都存在一个区域中心$<code>C_i|^k_&#123;i=1&#125;</code>$，这样对于每一个样本，都可以建立一种样本到区域中心的映射：  $<code>q(x)=\sum^k_&#123;i=1&#125;1(x&#123;\in&#125;S_i)C_i</code>$其中1()为指示函数，即代表样本x是否属于区域S</li>
<li>不同的基于划分的聚类算法的主要区别就在于如何建立相应的映射方式<code>q(x)</code>，在k-means中，映射是通过样本与各中心之间的误差平方和最小这一准则来建立的</li>
</ul>
</li>
<li>主要实现步骤：<ol>
<li>初始化聚类中心$<code>C_1^&#123;(0)&#125;， C_2^&#123;(0)&#125; ，C_3^&#123;(0)&#125;</code>$ …… $<code>C_k^&#123;(0)&#125;</code>$可选取前K个样本或者随机选取K个样本</li>
<li>分配各个样本$<code>x_j</code>$到距离最相近的聚类集合，样本分配依据为：$<code>S_i^&#123;(t)&#125;=\&#123;x_j\mid\|&#123;x_j-c_j^&#123;(t)&#125;\|_2^2&#125;\leq\|x_j-c_p^&#123;(t)&#125;\|_2^2\&#125;</code>$其中<code>i=1,2,…,k,p≠j</code></li>
<li>根据上一步的分配结果，更新聚类中心：$c_i^{(t+1)}=\dfrac{1}{|S_i^{(t)}|}\sum_{x_j\in{S_i^{(t)}}}x_j$</li>
<li>若迭代次数达到最大迭代步数或者前后两次迭代的差小于设定阈值ε<br>，即$<code>\|c_i^&#123;(t+1)&#125;-c_i^&#123;(t)&#125;\|_2^2&lt;\varepsilon</code>$，则算法结束，否则重复步骤2</li>
</ol>
</li>
</ul>
<p><img src="/2022/04/20/Kernel%20k-means,%20Spectral%20Clustering%20and%20Normalized/Untitled.png" alt="Untitled"></p>
<p>计算每个样本点簇中心的距离，然后判断出到哪个簇中心距离最短，然后分给那个簇。然后下次迭代时，簇中心就按照新分配的点重新进行计算了，然后所有的点再同样计算样本点到簇中心的距离，重新分配到不同的簇中。所以这样不断迭代下去，就能够收敛了，得到最后的聚类效果。</p>
<p>问题：对于非标准正态分布和非均匀分布的样本，k-means聚类效果不好，其主要原因是k-means是假设相似度可以用二次欧式距离等价的衡量，但是在实际的样本集合中这个假设不一定实用。</p>
<p>为了克服这个局限性，k-means需要推广到更广义的度量空间，其两种经典的改进框架如下：</p>
<h2 id="Kernel-k-means（核k-means方法）"><a href="#Kernel-k-means（核k-means方法）" class="headerlink" title="Kernel k-means（核k-means方法）"></a>Kernel k-means（核k-means方法）</h2><p>将样本点$<code>x_i</code>$通过某种映射方式$<code>x_i\rightarrow \phi(x_i)</code>$，映射到新的高维空间Φ<br>，在该空间中样本点之间的内积可以通过对应的核函数进行计算，即：</p>
<p>$<code>k(x_i,x_j)=\phi(x_i)^T\phi(x_j)</code>$</p>
<p>借助核函数，可以在新的空间进行k-means聚类，而此时样本之间的相似度取决于核函数的选择。</p>
<p><img src="/2022/04/20/Kernel%20k-means,%20Spectral%20Clustering%20and%20Normalized/Untitled%201.png" alt="Untitled"></p>
<p>ai表示样本点，mc表示每个簇的质心。</p>
<p>$\phi()$表示将样本点映射至高维空间。但这个函数是很难算出来的，因为这种非线性的函数映射，很难去得到准确的函数算法。且，簇的中心mc也很难去描述出来。</p>
<p>因此需要对这个函数进行改造：</p>
<p>将</p>
<p><img src="/2022/04/20/Kernel%20k-means,%20Spectral%20Clustering%20and%20Normalized/Untitled%202.png" alt="Untitled"></p>
<p>改为</p>
<p><img src="/2022/04/20/Kernel%20k-means,%20Spectral%20Clustering%20and%20Normalized/Untitled%203.png" alt="Untitled"></p>
<p>即：将该公式展开。</p>
<p>我们可以发现：每一项中，需要使用到高维特征空间的值，都是在进行内积运算。因此符合我们之前所提到的，使用一个矩阵K函数，来代表高维特征空间中这两个值的内积结果。</p>
<p>即：$<code>k(a_i,b_j)=\phi(a_i).\phi(b_j)</code>$</p>
<p>所以，我们只需要得到一个K矩阵，矩阵中每个元素都表示高维空间，这两个样本点映射成特征后，进行内积计算所得到的结果。所以这个矩阵，实际上就是一个<strong>核函数矩阵。</strong></p>
<h3 id="weighted-kernel-k-means"><a href="#weighted-kernel-k-means" class="headerlink" title="weighted kernel k-means"></a><strong>weighted kernel k-means</strong></h3><p>在<strong>kernel k-means</strong>基础上，为每一个样本点给出一个权重，以区分不同样本点的重要性。</p>
<p><img src="/2022/04/20/Kernel%20k-means,%20Spectral%20Clustering%20and%20Normalized/Untitled%204.png" alt="Untitled"></p>
<p>公式中计算样本高维空间特征到簇中心距离 的平方后，还乘以了一个权重$w_j$</p>
<h2 id="Spectral-Clustering（谱聚类方法）"><a href="#Spectral-Clustering（谱聚类方法）" class="headerlink" title="Spectral Clustering（谱聚类方法）"></a><strong><strong>Spectral Clustering（谱聚类方法）</strong></strong></h2><p>它的主要思想是把所有的数据看做空间中的点，这些点之间可以用边连接起来。距离较远的两个点之间的边权重值较低，而距离较近的两个点之间的边权重值较高，通过对所有数据点组成的图进行切图，让切图后不同的子图间边权重和尽可能的低，而子图内的边权重和尽可能的高，从而达到聚类的目的。</p>
<h1 id="摘要与背景"><a href="#摘要与背景" class="headerlink" title="摘要与背景"></a>摘要与背景</h1><hr>
<p>Kernel k-means和谱聚类都被用于识别在输入空间中非线性可分离的聚类。尽管进行了大量的研究，但这些方法仍然只是松散地联系在一起。</p>
<p>本文中，作者给出了它们之间明确的理论联系。证明了Weighted  kernel k-means目标函数的通用性，并推导出归一化切割的谱聚类目标作为一种特殊情况。</p>
<p>提出了一种新的Weighted kernel k-means算法（前提给出正定相似矩阵），其单调地减少了归一化割。</p>
<p>意义：</p>
<ul>
<li>基于特征向量的算法（可能在计算上是禁止的）对于最小化规范化切割不是必需的</li>
<li>各种技术，例如局部搜索和加速方案，可以用来提高核k-均值的质量和速度</li>
</ul>
<p>背景：</p>
<ul>
<li>聚类是数据挖掘的重要问题，k-means是最流行的聚类算法之一，受到了相关研究者的广泛关注。</li>
<li>k-means的一个主要缺点是，它不能分离在输入空间中非线性可分离的簇。当时的两种解决方法是：①Kernel k-means，在聚类之前，使用非线性函数将点映射到高维特征空间，然后Kernel k-means在新空间中用线性分隔符对点进行分割。②spectral clustering algorithms（谱聚类算法），使用亲和矩阵的特征向量来获得数据的聚类。谱聚类中常用的目标函数是最小化归一化切割的</li>
</ul>
<p>从表面上看，Kernel k-means和谱聚类似乎是完全不同的方法。</p>
<p>本文的目的：证明Kernel k-means和谱聚类之间的相关性（可以将加权核k-means目标函数重写为迹最大化问题）。这样当遇到需要计算大量特征向量的情景时，可以使用类似k-means的迭代算法来完成最小化图的归一划分。</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><ul>
<li>常见的非线性核函数</li>
</ul>
<p><img src="/2022/04/20/Kernel%20k-means,%20Spectral%20Clustering%20and%20Normalized/Untitled%205.png" alt="Untitled"></p>
<h3 id="Weighted-Kernel-k-means"><a href="#Weighted-Kernel-k-means" class="headerlink" title="Weighted Kernel k-means"></a>Weighted Kernel k-means</h3><p>k-means聚类算法可以通过使用核函数来增强；通过使用从原始（输入）空间到高维特征空间的适当非线性映射，可以提取在输入空间中非线性可分离的聚类。</p>
<p>此外，我们可以通过为每个点a引入一个权值来推广核k-means算法，用w(a)表示。</p>
<p>算法：</p>
<p><img src="/2022/04/20/Kernel%20k-means,%20Spectral%20Clustering%20and%20Normalized/Untitled%206.png" alt="Untitled"></p>
<p>第二项每个数据点计算一次，每次计算的成本是O(n)，导致每次迭代的成本是O(n2)。</p>
<p>因此，每次迭代的复杂度为O(n2)标量操作。首先，我们必须计算核矩阵K，这通常需要时间O(n2m)，其中m是原始点的维数。若总迭代次数为τ，则算法1的时间复杂度为O(n2(τ + m))。</p>
<h3 id="Spectral-clustering-谱聚类"><a href="#Spectral-clustering-谱聚类" class="headerlink" title="Spectral clustering(谱聚类)"></a>Spectral clustering(谱聚类)</h3><p>基本思想是利用样本数据的相似矩阵(拉普拉斯矩阵)进行特征划分后得到的特征向量进行聚类。<br>作者在本文重点研究归一化划分谱算法（normalized cut spectral algorithm）。<br>这里作者主要对另一篇论文(Multiclass spectral clustering)进行简单描述，将归一化划分准则等价于下列</p>
<p><img src="/2022/04/20/Kernel%20k-means,%20Spectral%20Clustering%20and%20Normalized/Untitled%207.png" alt="Untitled"></p>
<p>其中</p>
<p><img src="/2022/04/20/Kernel%20k-means,%20Spectral%20Clustering%20and%20Normalized/Untitled%208.png" alt="Untitled"></p>
<p>X是一个用于划分的n×k指示器矩阵</p>
<h1 id="THE-SPECTRAL-CONNECTION"><a href="#THE-SPECTRAL-CONNECTION" class="headerlink" title="THE SPECTRAL CONNECTION"></a>THE SPECTRAL CONNECTION</h1><p>乍一看，Weighted Kernel k-means和使用谱聚类的归一化切割似乎有很大的不同。毕竟，谱聚类使用特征向量来帮助确定分区，而特征向量没有出现在核k-means中。</p>
<p>然而，我们看到归一化切割问题可以表示为一个轨迹最大化(trace maximization)问题，在这一节中，我们展示了如何将Weighted Kernelk-means也表示为一个轨迹最大化问题。这将展示如何连接这两种聚类方法。</p>
<p>首先对πj 进行一个变形：</p>
<p><img src="/2022/04/20/Kernel%20k-means,%20Spectral%20Clustering%20and%20Normalized/Untitled%209.png" alt="Untitled"></p>
<p>$π_j$表示集群（簇）</p>
<p><img src="/2022/04/20/Kernel%20k-means,%20Spectral%20Clustering%20and%20Normalized/Untitled%2010.png" alt="Untitled"></p>
<p>之后定义：</p>
<p><img src="/2022/04/20/Kernel%20k-means,%20Spectral%20Clustering%20and%20Normalized/Untitled%2011.png" alt="Untitled"></p>
<p>其中$s_j$表示$π_j$的权重之和</p>
<p>$W$是所有点$w_i$权重的对角矩阵，$W_j$是$π_j$中权重的对角矩阵。</p>
<p>将均值向量$m_j$改为：</p>
<p><img src="/2022/04/20/Kernel%20k-means,%20Spectral%20Clustering%20and%20Normalized/Untitled%2012.png" alt="Untitled"></p>
<p>其中，W是所有点wi权重的对角矩阵，Wj是πj中权重的对角矩阵，Φj是与簇$π_j$相关的点矩阵，e是所有大小合适的向量</p>
<p>通过上述公式，可以将$π_j$的变形改为：</p>
<p><img src="/2022/04/20/Kernel%20k-means,%20Spectral%20Clustering%20and%20Normalized/Untitled%2013.png" alt="Untitled"></p>
<p>由于：</p>
<p><img src="/2022/04/20/Kernel%20k-means,%20Spectral%20Clustering%20and%20Normalized/Untitled%2014.png" alt="Untitled"></p>
<p>且：</p>
<p><img src="/2022/04/20/Kernel%20k-means,%20Spectral%20Clustering%20and%20Normalized/Untitled%2015.png" alt="Untitled"></p>
<p>是一个正交的投影，P$^2$=P，因为</p>
<p><img src="/2022/04/20/Kernel%20k-means,%20Spectral%20Clustering%20and%20Normalized/Untitled%2016.png" alt="Untitled"></p>
<p>因此又可以得到：</p>
<p><img src="/2022/04/20/Kernel%20k-means,%20Spectral%20Clustering%20and%20Normalized/Untitled%2017.png" alt="Untitled"></p>
<p>如果将点的完整矩阵表示为Φ=[Φ1，Φ2，…，Φk]，那么就得到：</p>
<p><img src="/2022/04/20/Kernel%20k-means,%20Spectral%20Clustering%20and%20Normalized/Untitled%2018.png" alt="Untitled"></p>
<p>Y是一个n×k的正交矩阵，即Y $^T$Y = I</p>
<p>由于trace($ΦWΦ^T$)是一个常数，所以我们加权核k-means的目标函数(（1）中目标函数的最小化）等价于trace($Y^T W ^1/2Φ^TΦW^1/2Y$)的最大化，其中$Φ^TΦ$就是数据的核矩阵K，因此可以化为trace($Y^T W ^1/2KW^1/2Y$)</p>
<h1 id="IMPLICATIONS"><a href="#IMPLICATIONS" class="headerlink" title="IMPLICATIONS"></a>IMPLICATIONS</h1><h3 id="Normalized-Cuts-using-Weighted-Kernel-k-means"><a href="#Normalized-Cuts-using-Weighted-Kernel-k-means" class="headerlink" title="Normalized Cuts using Weighted Kernel k-means"></a>Normalized Cuts using Weighted Kernel k-means</h3><p>使用加权核k均值的归一化切割</p>
<h3 id="Kernel-k-means-using-Eigenvectors"><a href="#Kernel-k-means-using-Eigenvectors" class="headerlink" title="Kernel k-means using Eigenvectors"></a>Kernel k-means using Eigenvectors</h3><p>使用特征向量的核k均值</p>
<h3 id="Interpreting-NJW-As-Kernel-k-means"><a href="#Interpreting-NJW-As-Kernel-k-means" class="headerlink" title="Interpreting NJW As Kernel k-means"></a>Interpreting NJW As Kernel k-means</h3><p>将NJW解释为核k-means</p>
<h1 id="EXPERIMENTAL-RESULTS"><a href="#EXPERIMENTAL-RESULTS" class="headerlink" title="EXPERIMENTAL RESULTS"></a>EXPERIMENTAL RESULTS</h1><p>首先用2度多项式核k均值说明基因的“直径聚类”</p>
<p>证明了用特征向量初始化核k-means可以得到更好的初始和最终目标函数值和更好的聚类结果。因此，谱聚类与核kmeans之间的理论联系有助于获得更高质量的结果。</p>
<p>最后，我们展示了我们的距离估计技术节省了大量的计算时间，验证了我们的方法的可扩展性。</p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><ul>
<li>Rosetta Inpharmatics的酵母数据集在300种条件下有5246个基因。</li>
<li>UCI机器学习库(<a href="ftp://ftp.ics.uci.edu/">ftp://ftp.ics.uci.edu/</a> pub/machine-learningdatabases/ Pendigits)下载，该库包含手写数字的(x, y)坐标。该数据集包含7494个训练数字和3498个测试数字。每个数字都表示为16维空间中的一个向量。</li>
</ul>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><ul>
<li>We first illustrate “diametric clustering” of genes with degree2 polynomial kernel k-means.<br>首先用2度多项式kernel k-means证明了基因的“相反聚类”</li>
<li>with the handwriting recognition data set, we show that using eigenvectors to initialize kernel k-means gives better initial and final objective function values and better clustering results.<br>利用手写识别数据集，证明了用特征向量初始化kernel k-means可以得到更好的初始和最终目标函数值以及更好的聚类结果。</li>
<li>谱聚类与kernel kmeans之间的理论联系有助于获得更高质量的结果。</li>
<li>最后，作者展示了距离估计技术节省了大量的计算时间，验证了方法的可扩展性</li>
</ul>
<h3 id="Diametrical-Clustering-of-Gene-Expression-Data（基因表达数据的聚类）"><a href="#Diametrical-Clustering-of-Gene-Expression-Data（基因表达数据的聚类）" class="headerlink" title="Diametrical Clustering of Gene Expression Data（基因表达数据的聚类）:"></a>Diametrical Clustering of Gene Expression Data（基因表达数据的聚类）:</h3><p><img src="/2022/04/20/Kernel%20k-means,%20Spectral%20Clustering%20and%20Normalized/Untitled%2019.png" alt="Untitled"></p>
<ul>
<li>在基因表达聚类中，识别基因之间的反相关关系是很重要的，因为已经观察到，表达模式强烈反相关的基因也可能在功能上相似。</li>
<li>结果：2次多项式kernel k-means捕获了与相反聚类算法捕获的反相关。与论文“Diametrical clustering for identifying anti-correlated gene clusters”结果一致</li>
</ul>
<h3 id="Clustering-Handwriting-Recognition-Data-Set（聚类笔迹识别数据集）"><a href="#Clustering-Handwriting-Recognition-Data-Set（聚类笔迹识别数据集）" class="headerlink" title="Clustering Handwriting Recognition Data Set（聚类笔迹识别数据集）"></a>Clustering Handwriting Recognition Data Set（聚类笔迹识别数据集）</h3><p>使用谱聚类的初始化kernel k-means输出通常可以产生比纯随机初始化更好的聚类结果。</p>
<p><img src="/2022/04/20/Kernel%20k-means,%20Spectral%20Clustering%20and%20Normalized/Untitled%2020.png" alt="Untitled"></p>
<p>NMI值越高，表示聚类和真实类标签匹配越好</p>
<p><img src="/2022/04/20/Kernel%20k-means,%20Spectral%20Clustering%20and%20Normalized/Untitled%2021.png" alt="Untitled"></p>
<p>同时，作者还提出了“三角不等式估计”（triangle inequality estimation mentioned）的方法，以此计算距离</p>
<p><img src="/2022/04/20/Kernel%20k-means,%20Spectral%20Clustering%20and%20Normalized/Untitled%2022.png" alt="Untitled"></p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>论文总结了传统的核k-means方法和谱聚类的方法，这两种方法看似是不相关的，但其实通过一定的公式推导和理论的证明，可以得到核k-means的方法也可以表达成为谱聚类那样的最大化迹的形式。</p>
<p>证明了weighted kernel k-means公式是非常通用的，并且归一化切割目标可以重铸为weighted kernel k-means 目标函数的一个特例。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">HotSpurzzZ</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2022/04/20/Kernel k-means, Spectral Clustering and Normalized/">http://example.com/2022/04/20/Kernel k-means, Spectral Clustering and Normalized/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com">HotSpurzzZ</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Paper-Reading/">Paper Reading</a><a class="post-meta__tags" href="/tags/Data-mining/">Data mining</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2022/05/01/00-LLVM%E6%BA%90%E7%A0%81%E5%AE%89%E8%A3%85/"><i class="fa fa-chevron-left">  </i><span>00-LLVM源码安装</span></a></div><div class="next-post pull-right"><a href="/2022/04/04/Pande%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/"><span>Pande使用：record与replay</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: 'b166e2f551e9803e7eef',
  clientSecret: '9267242d46a5b1af315666cd638e6db8e0bb8249',
  repo: 'blogcomments',
  owner: 'HotSpurzzZ',
  admin: 'HotSpurzzZ',
  id: md5(decodeURI(location.pathname)),
  language: 'zh-CN'
})
gitalk.render('gitalk-container')</script></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2023 By HotSpurzzZ</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>